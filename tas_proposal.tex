% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage[numbers]{natbib}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{color}
\newcommand{\abc}[1]{{\color{red} #1}}
\usepackage{amsmath}

\begin{document}

\conferenceinfo{}{June 2016}


%
% --- Author Metadata here ---
% \conferenceinfo{}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{A Question--Answer Selection Model Proposal}
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{Alberto Barr\'on-Cede\~no}
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.


\maketitle
\begin{abstract}
\end{abstract}

%%%%%
% One of the features our product offers is "macro suggestions". That is, we 
% suggest the most suitable answers to an unanswered question, out of a set of 
% templated responses/answers (macros). In order to build our model, we look at 
% the previous history of tickets (questions & answers that a customer service 
% team has received and solved) and look for previous answers that are similar to 
% any of the templated responses we have.
% 
% There are a few situations to consider for this problem:
% - First of all, a reply doesn't necessarily belong to any templated answer 
% class.
% - In other cases, only part of the reply will contain a templated response.
% - Finally, very often a templated response will have been used but slightly 
% modified to suit the question, the tone of the conversation or the users' 
% data better.
% 
% a) Given a set of templated responses (macros) and an answer chosen from a 
% ticket at random, describe a method that determines whether this answer belongs 
% to or is based on one of the templated responses, and if that's the case 
% classifies it into the right macro.
% 
% Now that we are able to find and put into a certain class the questions that 
% trigger a macro response, we can build our macro classifier for suggesting 
% macros to new unanswered questions.
% 
% b) Assuming you have a dataset of questions, where each one is labelled with the 
% macro class it belongs to, describe one method you would explore to create a 
% classifier that can label new, previously unseen questions. Also, don't forget 
% to explain how you'd engineer the features used to learn your classifier.
% 
% Note: We can assume the training set has between 1,000 and 100,000 instances, 
% and between 10 and 1000 classes.

%%%%%

%
%  Use this command to print the description
%
%\printccsdesc

% We no longer use \terms command
%\terms{Theory}
%\vspace{-1em}
%\keywords{Community Question Answering; Learning to Rank, Syntactic Structures}


\section{Introduction}
\label{sec:intro}


% \section{Related Work}
\section{Problem Description} 
% \label{sec:taskdescription}


\subsection{Problem Constraints}

In the problem of suggesting answers out of a pool of question--answer pairs, a 
few constraints must be considered:

\begin{itemize}
\item 
\end{itemize}





\section{Model(s) to Classify Answers}
\label{sec:answers}
%  a) Given a set of templated responses (macros) and an answer chosen from a 
% ticket at random, describe a method that determines whether this answer 
% belongs to or is based on one of the templated responses, and if that's the 
% case % classifies it into the right macro
The collection of the so called macros consist of question-answer pairs $q,a$. 
The proposed problem is as follows: given a new answer $a'$, extracted from a 
ticket, and a templated answer $a$,  determine whether $a'$ belongs to the macro 
class. 

Following a straight-forward approach in which each macro belongs to a class 
seems not such a good idea, as not enough information would exist to model 
each class. Instead, I would opt for a paraphrase-like approach. That is, given 
$a$, and $a'$, are they a paraphrase of each other? In a more relaxed 
definition, do $a$ and $a'$ convey the same information? This approach would 
allow to pass from a scenario in which there are as many classes as instances 
into a binary one: are these answers related or not? Obviously, under this 
setting the questions' information can be used as well: whether $a,a'$ are 
related is not the only relevant aspect. Also whether $q,q'$ are. 

Another ``flavour'' of the proposed problem is that of determining whether a 
answer ticket answer is ``based'' on a templated one. This problem is more 
similar to that text re-use identification (which most ``famous'' variant is 
that of plagiarism detection). In this case, the problem is, given text $a$ and 
$a'$, is $a'$ derived from the contents in $a$? Common approaches to this 
problem consist of a direct comparison of flexible, still expressive, 
representations of the texts. For instance, simple representations based on 
characters or word $n$-grams have shown remarkable performance~\cite{Lyon:04}. 
More sophisticated models use dotplot~\cite{Basile:2009} and even semantic 
models~\cite{Gabrilovich:07}, also across languages~\cite{Potthast:11}. 
Obviously, a combination of these representations can be fed into a machine 
learning algorithm to learn  to differentiate between potential derivation or 
not, or simply to generate a ranking with the most likely candidates.% 
\footnote{An overview of many models for text re-use detection can be found 
in~\cite{BarronPhd:12}.}

ch class question $q'$, t seems like 
in this 


\section{Model(s) to Classify Questions}
% b) Assuming you have a dataset of questions, where each one is labelled with
% the macro class it belongs to, describe one method you would explore to create 
% a classifier that can label new, previously unseen questions. Also, don't 
% forget to explain how you'd engineer the features used to learn your 
% classifier.
Problem ``b'' can be formally defined as follows. Let $q\in c_i$ be a previously 
observed question that belongs to macro-class $c_i$%
\footnote{What exactly means macro-class (opposite to a simple class) should be 
discussed.}
Let $Q$ be a collection of questions, each of which belongs to class $c\in C_I$. 
Build a classifier which predicts the most likely class of a new question $q_n$. 
That is, this problems falls into the classical multi-class scenario. Multiple 
machine learning models exist to approach this kind of problem, ranging from 
simple decision trees to more sophisticated convolutional models.%
\footnote{Maybe remove this ``fancy'' convolutional terms}
The distribution of the classes is currently unknown, which causes the selection 
of the best model difficult. For instance, if there was a dominant class, that 
is, a class with a highly representative amount of instances in the training 
set, even a decision tree could be considered, as this kind of model do well 
with imbalanced datasets. Still, the first option seems to be to apply a 
multi-class support vector machine (SVM)~\cite{Joachims:99}; for 
instance, in the form of an ensemble of binary classifiers. That is, $I$ binary 
SVMs are built, one per class, and the predicted class becomes the one with the 
maximum score among all the classifiers. 

It is worth noting that this is not posed as a paraphrasing problem. That is, 
the task is not determining whether $q_i$ and $q_j$ are similar to each other. 
Instead, the task consists of determining whether the new question $q$ belongs 
to the same macro-class as a set of questions $Q$. As a result, it is not 
unreasonable to approach the problem with models such as $k$ nearest neighbors. 
Of course, before opting for such a model, it would be necessary to prove before 
hand if a characterisation is at hand which actually causes the instances that 
belong to the same class to fall close within the vector space. 

\subsection{Preliminary Feature Engineering}

Going back to the differentiation between the proposed task and paraphrasing, 
if this task was the latter, the core of the features would be composed of a 
manifold of similarities at lexical, syntactic, and semantic level. Of course, 
in our case such similarities would still play an important role in the 
inference ---if $q_1$ belongs to class $i$ and $sim(q_1,q_2)\rightarrow 1$, it 
is very likely that both questions belong to the same class. 

Another bunch of features I would consider would consist of the so called 
explicit semantic analysis (ESA)~\cite{Gabrilovich:07}. In summary, 
ESA takes advantage of a large reference corpus, usually containing a manifold 
of encyclopedic articles. ESA represents a text as a vector in which each 
dimension is activated by the similarity between the text and the article 
contents. As a result, identifying the topics covered in a question might 
represent valuable information to determine if it belongs to a given 
macro-class. Obviously, ESA could be computed both at question and at word 
label. In the later case, the representation vector slice would consists of 
average over all the words in $q$.

Another subset of features would be vocabulary-based. With statistical weighting 
models such as the well known $tf$-$idf$, I would identify the likelihood of a 
given keyword of belonging to given class. In a clustering-like fashion, we can 
identify the keywords with the highest and lowest association levels to each of 
the classes. A selection of the most discriminating keywords would be considered 
as binary features: whether keyword $k$ appears or not in the question. 

% case we are more interested in the subject, in the topic of the class. 
% Therefore, in principle I would opt for ass or which be definition of tven a set 
% of questions $Q$ belonging to  represents 
% Both the answer and question identification problems can be defined as a ranking 
% task. Given a new question $q$, retrieve the most similar (relevant) question 
% (answer) from a pool of previously existing uly exosn the case of 

\section{Learning Strategy}
\label{sec:learning}
% Note: We can assume the training set has between 1,000 and 100,000 instances, 
% and between 10 and 1000 classes.
The problem exposes different potential scenarios, which depend on two 
parameters: number of instances and number of classes. The range of instances 
lies in the range $s_i\in [1,000, 10,000]$, whereas the number of classes lies 
in the range $s_c\in [10,1000]$. I will discard the extreme cases in which 
$s_i\approx s_c$ as a straight learning strategy would hardly learn anything and 
the problem could be reduced to a 1-nearest-neighbor decision taking. In this 
case, some structure could be discovered by hierarchical clustering of the 
training data, which could help to find the most similar instances to the new 
one. 

Beside these extreme cases, I will explore the potential strategies depending on 
the two variables in isolation. Let us consider $s_i$: the number of instances. 
For relatively small amounts of data, that is, a few thousand, I would 
definitively opt for $n$-fold cross validation (even hold out). This would allow 
for tuning the parameters of the model and get more stable estimations of the 
outcomes when facing new instances. The closer to the $10,000$ extreme, the more 
likely I would opt for a standard three-set partition: training, development, 
and test. 

Regarding $s_c$, the number of classes, let us assume, just for a moment, that 
we opt for using support vector machines. I would opt for the standard 
multi-class setting based in $s_c$ one-vs-the-rest classifiers. Hundreds of 
classes would require other strategies, such as 
DAGSVM~\cite{large-margin-dags-for-multiclass-classification}. In this kind of 
strategy, $s_c(s_c-1)/2$ (and not $s_c^2$!) binary classifiers are arranged 
within a direct acyclic graph. I used a variation of this strategy some years 
ago, when facing the standard 20 Newsgroups dataset;%
\footnote{\url{http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html
}}
still I consider that it could properly scale to hundreds of classes. Different 
parallelisation strategies could be applied to run plenty of these classifiers 
at reasonable temporal cost, even if on commodity hardware.


Other more sophisticated strategies have been proposed, such 
as~\cite{JMLR:v15:gupta14a}, which could be explored.%
\footnote{Still I have to admit I am not familiar with this kind of technique.}
Observe that, in order to converge to reasonable models, millions of instances 
are usually required to when dealing with thousands of classes (cf. Table~4 
in~\cite{JMLR:v15:gupta14a}).%
\footnote{I have observed that deep learning approaches (e.g., deep 
neural networks) promise to deal with problems with thousands of classes as 
well. Still, as in the discussed case, they require huge amounts of 
data.}
% ive ven if the which mall amounts exist for those cases in I would opt for 
% a multiclass-settingven sim1,000, as th classeshe keywords gain,four 

\section{Implementation Specificities (potential section)}
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% \appendix
% %Appendix A
% \section{Headings in Appendices}
% The sig-alternate.cls file itself is chock-full of succinct
% and helpful comments.  If you consider yourself a moderately
% experienced to expert user of \LaTeX, you may find reading
% it useful but please remember not to change it.
% %\balancecolumns % GM June 2007
% % That's all folks!
\end{document}


