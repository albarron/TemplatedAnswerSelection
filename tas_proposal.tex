%
% File ACL2016.tex
%

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{latexsym}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}

\usepackage{url}
\usepackage{color}
\usepackage{amsmath}
\usepackage{graphicx}


\newcommand\BibTeX{B{\sc ib}\TeX}



\newcommand{\alex}[1]{{\color{red} #1}}
\newcommand{\bluett}[1]{\textcolor{blue}{\tt #1}}
\newcommand{\abc}[1]{\textcolor{magenta}{#1}}
\newcommand{\gio}[1]{\textcolor{green}{#1}}

\newcommand{\cf}{cf.~}
\newcommand{\ie}{i.e.,~}
\newcommand{\eg}{e.g.,~}
%\setlength\titlebox{5cm}
\setlength\titlebox{3.8cm}

\newcommand{\Ni}{({\em i})~}
\newcommand{\Nii}{({\em ii})~}
\newcommand{\Niii}{({\em iii})~}
\newcommand{\Niv}{({\em iv})~}
\newcommand{\Nv}{({\em v})~}
\newcommand{\Nvi}{({\em vi})~}

\usepackage{xspace} 

\newcommand{\good}{\emph{good}\xspace}
\newcommand{\bad}{\emph{bad}\xspace}
\newcommand{\dial}{\emph{dialogue}\xspace}
\newcommand{\pot}{\emph{potential}\xspace}
\newcommand{\rest}{\emph{rest}\xspace}

\newcommand{\goods}{\emph{good}}
\newcommand{\bads}{\emph{bad}}
\newcommand{\pots}{\emph{potential}}


\title{A Question--Answer Selection Model Proposal}



\author{Alberto Barr\'on-Cede\~no}
\date{}

\begin{document}
\maketitle
% \begin{abstract}
% \end{abstract}

%%%%%
% One of the features our product offers is "macro suggestions". That is, we 
% suggest the most suitable answers to an unanswered question, out of a set of 
% templated responses/answers (macros). In order to build our model, we look at 
% the previous history of tickets (questions & answers that a customer service 
% team has received and solved) and look for previous answers that are similar to 
% any of the templated responses we have.
% 
% There are a few situations to consider for this problem:
% - First of all, a reply doesn't necessarily belong to any templated answer 
% class.
% - In other cases, only part of the reply will contain a templated response.
% - Finally, very often a templated response will have been used but slightly 
% modified to suit the question, the tone of the conversation or the users' 
% data better.
% 
% a) Given a set of templated responses (macros) and an answer chosen from a 
% ticket at random, describe a method that determines whether this answer belongs 
% to or is based on one of the templated responses, and if that's the case 
% classifies it into the right macro.
% 
% Now that we are able to find and put into a certain class the questions that 
% trigger a macro response, we can build our macro classifier for suggesting 
% macros to new unanswered questions.
% 
% b) Assuming you have a dataset of questions, where each one is labelled with the 
% macro class it belongs to, describe one method you would explore to create a 
% classifier that can label new, previously unseen questions. Also, don't forget 
% to explain how you'd engineer the features used to learn your classifier.
% 
% Note: We can assume the training set has between 1,000 and 100,000 instances, 
% and between 10 and 1000 classes.

\section{Definitions}
\label{sec:def}

\begin{description}
\item{Macro.} Also known as templated response or answer. Is a previously 
existing answer that is part of a repository of answers.
\item{Ticket.} Question--answer pair that a customer service team has received, 
and solved. 
\end{description}


% \section{Related Work}
% \section{Problem Description} 
% \label{sec:taskdescription}


% \subsection{Problem Constraints}
% 
% In the problem of suggesting answers out of a pool of question--answer pairs, a 
% few constraints must be considered:
% 
% \begin{itemize}
% \item 
% \end{itemize}

\section{Model(s) to Classify Answers}
\label{sec:answers}

Problem: Given a set of templated responses (macros) and an answer chosen from a 
ticket at random, describe a method that determines whether this answer belongs 
to or is based on one of the templated responses, and if that's the case 
classifies it into the right macro.
\medskip

The collection of the so called macros consist of answers $a\in A$. The proposed 
problem is as follows: given a new answer $a'$, extracted from a ticket, and a 
templated answer $a$,  determine whether $a'$ belongs to the macro class. 

Following a straight-forward approach in which each macro belongs to a class 
seems not such a good idea, as not enough information would exist to model each 
class. As argued in Section~\ref{sec:learning}, when learning on multiple 
classes, the necessary amount of instances tends to be huge. Instead, I would 
opt for a paraphrase-like approach. That is, given $a$, and $a'$, are they a 
(partial) paraphrase of each other? In a more relaxed fashion, do $a$ and 
$a'$ convey the same information? This approach would allow to pass from a 
scenario in which there are as many classes as instances into a binary one: are 
these answers related or not? Obviously, under this setting the questions' 
information can be used as well: whether $(a,a')$ are related is not the only 
relevant information to make a decision; also whether $(q,q')$ are. 
%Also, once the binary problem is solved, the new question can of course be 
% added to the macro bin.

Another ``flavour'' of the proposed problem is that of determining whether a 
ticket answer is ``based'' on a templated one. This is closer to that of 
text re-use identification; an information retrieval problem.  
% (which most ``famous'' variant is that of plagiarism detection). 
In this case, the problem is, given texts $a$ and $a'$, is $a'$ derived from the 
contents in $a$? Common approaches to this problem consist of a direct 
comparison of flexible, still expressive, representations of the texts. For 
instance, simple representations based on characters or word $n$-grams have 
shown remarkable performance~\cite{Lyon:04}. More sophisticated models use 
dotplot techniques~\cite{Basile:2009} and even semantic 
representations~\cite{Gabrilovich:07}, also across languages~\cite{Potthast:11}. 
Obviously, a combination of these representations can be fed into a machine 
learning algorithm to learn to differentiate between potential derivation or 
not, or simply to generate a ranking with the most likely candidates.% 
% \footnote{An overview of many models for text re-use detection can be .}

Of course a simple out-of-the-box option to consider would be a standard search 
engine. Perhaps this approach would require slight modifications to the ranking 
function. The reason is that we are not talking about a standard information 
retrieval problem, in which we have a set of documents and a (short) query. This 
is a retrieve-by-example problem: we have documents in the index (answers) and 
our query is another document (another answer). 
\medskip

I have experience in all these approaches. From the identification of 
paraphrases to the identification of derived and re-used texts. See for 
instance~\cite{BarronPhd:12}.

\section{Model(s) to Classify Questions}

Problem: assuming you have a dataset of questions, where each one is labelled 
with the macro class it belongs to, describe one method you would explore to 
create a classifier that can label new, previously unseen questions. Also, don't 
forget to explain how you'd engineer the features used to learn your classifier.
\medskip

This problem can be formally defined as follows. Let $q\in c_i$ be a previously 
observed question that belongs to macro-class $c_i$.%
\footnote{What exactly macro-class means (opposite to a simple class) is open 
to discussion. I am assuming the prefix macro stands for the macros in the 
collection and not for a class which contains plenty of sub-classes.}
Let $Q$ be a collection of questions, each of which belongs to one class $c_i$, 
$i=[1,\ldots,I]$. Build a classifier that predicts the most likely class of a 
new question $q_n$. That is, this is a multi-class classification problem. 
Multiple machine learning models exist to approach this kind of problem, ranging 
from simple decision trees to more sophisticated probabilitic models. %
% \footnote{\abc{Maybe remove this ``fancy'' convolutional terms}}
The distribution of the classes is currently unknown, which causes the selection 
of the best model difficult. For instance, if there was a dominant class, that 
is, a class with a highly representative amount of instances in the training 
set, even a decision tree could be considered, as this kind of models do well 
with imbalanced datasets. Still, the first option seems to be to apply a 
multi-class support vector machine (SVM)~\cite{Joachims:99}; for 
instance, in the form of an ensemble of binary classifiers. That is, $I$ 
one-vs-the-rest binary SVMs are built, one per class, and the predicted class 
becomes the one with the maximum score among all the classifiers. 

It is worth noting that this is not posed as a paraphrasing problem. That is, 
the task is not determining whether $q_i$ and $q_j$ are similar to each other. 
Still, similarly to the idea discussed in Section~\ref{sec:answers}, the 
classes could become bins and $q_n$ would be assigned to that ``macro-class'' 
with the instances which are most-likely paraphrases of $q_n$. Going 
one step back, as the task consists of determining whether the new 
question $q_n$ belongs to the same macro-class as a set of questions $Q$, it is 
not unreasonable to approach the problem with models such as $k$ nearest 
neighbours. Of course, before opting for such a model, it would be necessary to 
prove beforehand if a characterisation is at hand which actually causes the 
instances that belong to the same class to fall close within the representation 
space. 

\subsection{Preliminary Feature Engineering}

Going back to the differentiation between the proposed task and paraphrasing, 
if this task was the latter, the core of the features would be composed of a 
manifold of similarities at lexical, syntactic, and semantic level. Of course, 
in our case such similarities would still play an important role in the 
inference ---if $q_1$ belongs to class $i$ and $sim(q_1,q_2)\rightarrow 1$, it 
is very likely that both questions belong to the same class. 

Another set of features worth considering is derived from the so called explicit 
semantic analysis (ESA)~\cite{Gabrilovich:07}. In brief, ESA takes advantage of 
a large reference corpus ---usually containing a manifold of encyclopedic 
articles---, and represents a text as a vector in which each dimension is 
activated by the similarity between the text and the article contents. 
Identifying the topics covered in a question might represent valuable 
information to determine if it belongs to a given macro-class. The similarity 
between the ESA representations of the two implied questions would play the role 
of yet another feature. ESA could be computed both at question and at word 
label. In the later case, the representation vector slice would consist of the 
average over all the words in $q$.

Another subset of features would be vocabulary-based. With statistical weighting 
models such as the well known $tf$-$idf$, I would identify the likelihood of a 
given keyword of belonging to each class; that is, $p(k\mid c)$. In a 
clustering-like fashion, we can identify the keywords with the highest and 
lowest association levels to each of the classes. A selection of the most 
discriminating keywords would be considered as binary features: whether keyword 
$k$ appears or not in the question. Indeed, some approaches exist that consider 
each token in the text as a feature. Nevertheless, I am not particularly in 
favour of such approaches, as the representations tend to become highly sparse 
and the achieved improvement tends to be fairly small.
\medskip

I have been working on very similar problems as this one during the last two 
years. The papers on this topic are currently under review.

% case we are more interested in the subject, in the topic of the class. % 
% Therefore, in principle I would opt for ass or which be definition of tven a set 
% of questions $Q$ belonging to  represents 
% Both the answer and question identification problems can be defined as a ranking 
% task. Given a new question $q$, retrieve the most similar (relevant) question 
% (answer) from a pool of previously existing uly exosn the case of 

\section{Learning Strategy}
\label{sec:learning}
% Note: We can assume the training set has between 1,000 and 100,000 instances, 
% and between 10 and 1000 classes.
Regardless of the task, different scenarios have been proposed and now I discuss 
on them. The potential scenarios depend on two parameters: number of instances 
and number of classes. The number of instances lies in the range $s_i\in [1k, 
100k]$, whereas the number of classes lies in the range $s_c\in [10, 1k]$. 

I will discard the extreme cases in which $s_i\approx s_c$ as a straight 
learning strategy would hardly learn anything and the problem could be reduced 
to a $1$-nearest-neighbour decision taking. In this case, some structure could 
be discovered by hierarchical clustering of the training data, which could help 
to find the most similar instances against the new one. That is, if there 
where classes within other super-classes, the structure of such network could 
be exploited.

Beside the extreme cases, I will explore the potential strategies depending on 
the two variables in isolation. Let us consider $s_i$: the number of instances. 
For relatively small amounts of data, that is, a few thousand, I would 
definitively opt for an $n$-fold cross validation (even hold out) learning 
strategy. This would allow for tuning the 
parameters of the model and get more stable estimations of the outcomes when 
facing new instances. The closer to the $100k$ extreme, the more likely I 
would opt for a standard three-set partition: training, development, and test. 
Of course, this would not prevent the use of cross-validation on the 
training set. 

Regarding $s_c$, the number of classes, let us assume, just for a moment, that 
we opt for using support vector machines. I would opt for the standard 
multi-class setting based on $s_c$ one-vs-the-rest classifiers. Hundreds of 
classes would require other strategies, such as 
DAGSVM~\cite{large-margin-dags-for-multiclass-classification}. In this kind of 
strategy, $s_c(s_c-1)/2$ (and not $s_c^2$!) binary classifiers are arranged 
within a direct acyclic graph. I used a variation of this strategy some years 
ago, when facing the standard 20 Newsgroups dataset;%
\footnote{\url{http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html
}}
still I consider that it could properly scale to hundreds of classes. Different 
parallelisation strategies could be applied to run plenty of these classifiers 
at reasonable temporal cost, even if on commodity hardware.


Other more sophisticated strategies have been proposed, such 
as~\cite{JMLR:v15:gupta14a}, which could be explored.%
\footnote{I have to admit I am not familiar with these techniques.}
Observe that, in order to converge to reasonable models, huge amounts of 
instances are usually required when dealing with thousands of classes (cf. 
Table~4 in~\cite{JMLR:v15:gupta14a}, where they use millions). Therefore, I 
would try such strategies only if the number of instances gets closer to 
$100k$ and, hopefully, larger.%
\footnote{I have observed that deep learning approaches (e.g., deep 
neural networks) promise to deal with problems with thousands of classes as 
well. Still, as in the discussed case, they require huge amounts of 
data.}
% ive ven if the which mall amounts exist for those cases in I would opt for 
% a multiclass-settingven sim1,000, as th classeshe keywords gain,four 

% \section{Implementation Specificities (potential section)}
%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
% \bibliographystyle{abbrv}
\bibliographystyle{acl2016}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns
% \appendix
% %Appendix A
% \section{Headings in Appendices}
% The sig-alternate.cls file itself is chock-full of succinct
% and helpful comments.  If you consider yourself a moderately
% experienced to expert user of \LaTeX, you may find reading
% it useful but please remember not to change it.
% %\balancecolumns % GM June 2007
% % That's all folks!
\end{document}


